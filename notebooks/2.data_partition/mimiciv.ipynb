{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/python/myenv/medical-coding-reproducibility-main\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('../../')\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "from pathlib import Path\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "\n",
    "from src.settings import (\n",
    "    DATA_DIRECTORY_MIMICIV_ICD10,\n",
    "    ID_COLUMN,\n",
    "    SUBJECT_ID_COLUMN,\n",
    "    TARGET_COLUMN,\n",
    ")\n",
    "\n",
    "from src.utils.stratify_function import (\n",
    "    iterative_stratification,\n",
    "    kl_divergence,  \n",
    "    labels_not_in_split,\n",
    ")\n",
    "\n",
    "\n",
    "# Generate splits\n",
    "def generate_split(dataset: pd.DataFrame, split_path: Path):\n",
    "    processlog=[]\n",
    "    splits = dataset[[SUBJECT_ID_COLUMN, ID_COLUMN]]\n",
    "    subject_series = dataset.groupby(SUBJECT_ID_COLUMN)[TARGET_COLUMN].sum()\n",
    "    subject_ids = subject_series.index.to_list()\n",
    "    codes = subject_series.to_list()\n",
    "    subject_ids_train, subject_ids_test = iterative_stratification(\n",
    "        subject_ids, codes, [1 - TEST_SIZE, TEST_SIZE]\n",
    "    )\n",
    "    codes_train = [\n",
    "        codes[subject_ids.index(subject_id)] for subject_id in subject_ids_train\n",
    "    ]\n",
    "    val_size = VAL_SIZE / (1 - TEST_SIZE)\n",
    "    subject_ids_train, subject_ids_val = iterative_stratification(\n",
    "        subject_ids_train, codes_train, [1 - val_size, val_size]\n",
    "    )\n",
    "\n",
    "    codes_train = [\n",
    "        codes[subject_ids.index(subject_id)] for subject_id in subject_ids_train\n",
    "    ]\n",
    "    codes_val = [codes[subject_ids.index(subject_id)] for subject_id in subject_ids_val]\n",
    "    codes_test = [\n",
    "        codes[subject_ids.index(subject_id)] for subject_id in subject_ids_test\n",
    "    ]\n",
    "\n",
    "    splits.loc[splits[SUBJECT_ID_COLUMN].isin(subject_ids_train), \"split\"] = \"train\"\n",
    "    splits.loc[splits[SUBJECT_ID_COLUMN].isin(subject_ids_val), \"split\"] = \"val\"\n",
    "    splits.loc[splits[SUBJECT_ID_COLUMN].isin(subject_ids_test), \"split\"] = \"test\"\n",
    "\n",
    "    processlog.append(logging.info(\"------------- Splits Statistics -------------\"))\n",
    "    processlog.append(logging.info(\n",
    "        f\"Labels missing in the test set: {labels_not_in_split(codes, codes_test)}\"\n",
    "    ))\n",
    "    processlog.append(logging.info(\n",
    "        f\"Labels missing in the val set: {labels_not_in_split(codes, codes_val)} %\"\n",
    "    ))\n",
    "    processlog.append(logging.info(\n",
    "        f\"Labels missing in the train set: {labels_not_in_split(codes, codes_train)} %\"\n",
    "    ))\n",
    "    processlog.append(logging.info(f\"Test: KL divergence: {kl_divergence(codes, codes_test)}\"))\n",
    "    processlog.append(logging.info(f\"Val: KL divergence: {kl_divergence(codes, codes_val)}\"))\n",
    "    processlog.append(logging.info(f\"Train: KL divergence: {kl_divergence(codes, codes_train)}\"))\n",
    "    processlog.append(logging.info(f\"Test Size: {len(codes_test) / len(codes)}\"))\n",
    "    processlog.append(logging.info(f\"Val Size: {len(codes_val) / len(codes)}\"))\n",
    "    processlog.append(logging.info(f\"Train Size: {len(codes_train) / len(codes)}\"))\n",
    "\n",
    "    splits = splits[[ID_COLUMN, \"split\"]].reset_index(drop=True)\n",
    "    splits.to_feather(split_path)\n",
    "    processlog.append(logging.info(\n",
    "        \"Splits generated and saved. Now making subsplits used to analyse the performance of the models when trained on less data.\"\n",
    "    ))\n",
    "    return processlog\n",
    "\n",
    "def generate_training_subset(\n",
    "    dataset: pd.DataFrame,\n",
    "    splits: pd.DataFrame,\n",
    "    number_of_training_examples: int,\n",
    "    split_path: Path,\n",
    "):\n",
    "    processlog = []\n",
    "    # Merge the dataset with splits\n",
    "    dataset = pd.merge(dataset, splits, on=ID_COLUMN)\n",
    "    \n",
    "    # Separate the dataset into train, validation, and test sets\n",
    "    \n",
    "    training_set = dataset[dataset[\"split\"] == \"train\"]\n",
    "    val_set = dataset[dataset[\"split\"] == \"val\"]\n",
    "    test_set = dataset[dataset[\"split\"] == \"test\"]\n",
    "\n",
    "    \n",
    "    # Calculate the size for stratification\n",
    "    size = number_of_training_examples / len(training_set)\n",
    "\n",
    "    # Group by subject ID and compute target column sums\n",
    "    subject_series = training_set.groupby(SUBJECT_ID_COLUMN)[TARGET_COLUMN].sum()\n",
    "    subject_ids = subject_series.index.to_list()\n",
    "    codes = subject_series.to_list()\n",
    "\n",
    "    # Perform iterative stratification\n",
    "    _, subject_ids_train_subset = iterative_stratification(\n",
    "        subject_ids, codes.copy(), [1 - size, size]\n",
    "    )\n",
    "    \n",
    "    # Retrieve the stratified training subset\n",
    "    codes_train_subset = [\n",
    "        codes[subject_ids.index(subject_id)] for subject_id in subject_ids_train_subset\n",
    "    ]\n",
    "    \n",
    "    # Append logging information individually\n",
    "    processlog.append(logging.info(f\"------------- Splits Statistics {number_of_training_examples}-------------\"))\n",
    "    processlog.append(logging.info(\n",
    "        f\"Labels missing in the training subset: {labels_not_in_split(codes, codes_train_subset)} %\"\n",
    "    ))\n",
    "    processlog.append(logging.info(\n",
    "        f\"Train subset: KL divergence: {kl_divergence(codes, codes_train_subset)}\"\n",
    "    ))\n",
    "    processlog.append(logging.info(f\"Train subset size: {len(codes_train_subset) / len(codes)}\"))\n",
    "\n",
    "    # Filter the training set with the stratified subset of subject IDs\n",
    "    training_set = training_set[\n",
    "        training_set[SUBJECT_ID_COLUMN].isin(subject_ids_train_subset)\n",
    "    ]\n",
    "    \n",
    "    # Concatenate the updated training set with validation and test sets\n",
    "    dataset = pd.concat([training_set, val_set, test_set])[\n",
    "        [ID_COLUMN, \"split\"]\n",
    "    ].reset_index(drop=True)\n",
    "\n",
    "    # Save the resulting dataset\n",
    "    dataset.to_feather(split_path)\n",
    "\n",
    "    return processlog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KL - Entropy น้อยๆ คือ กระจายทั่วถึง"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.15  # Test split ratios\n",
    "VAL_SIZE = 0.1\n",
    "STEP_SIZE = 0.2  # Step size for the iterative stratification\n",
    "\n",
    "random.seed(10)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "output_dir_icd10 = Path(DATA_DIRECTORY_MIMICIV_ICD10)\n",
    "\n",
    "\n",
    "mimic_icd10 = pd.read_feather(output_dir_icd10 / \"mimiciv_icd10.feather\")\n",
    "mimic_icd10[TARGET_COLUMN] = mimic_icd10[TARGET_COLUMN].apply(lambda x: list(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_83426/4175313070.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  splits.loc[splits[SUBJECT_ID_COLUMN].isin(subject_ids_train), \"split\"] = \"train\"\n",
      "INFO:root:------------- Splits Statistics -------------\n",
      "INFO:root:Labels missing in the test set: 0.08813900780659784\n",
      "INFO:root:Labels missing in the val set: 0.45328632586250317 %\n",
      "INFO:root:Labels missing in the train set: 0.03777386048854193 %\n",
      "INFO:root:Test: KL divergence: 0.0043161456458808475\n",
      "INFO:root:Val: KL divergence: 0.00741076150597404\n",
      "INFO:root:Train: KL divergence: 0.00042690198364618505\n",
      "INFO:root:Test Size: 0.15981297023972707\n",
      "INFO:root:Val Size: 0.10798379481555942\n",
      "INFO:root:Train Size: 0.7322032349447135\n",
      "INFO:root:Splits generated and saved. Now making subsplits used to analyse the performance of the models when trained on less data.\n"
     ]
    }
   ],
   "source": [
    "tasks = [\n",
    "    (mimic_icd10, output_dir_icd10 / \"mimiciv_icd10_split.feather\")\n",
    "]\n",
    "\n",
    "with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:\n",
    "    results = pool.starmap(generate_split, tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_icd10_splits = pd.read_feather(output_dir_icd10 / \"mimiciv_icd10_split.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
