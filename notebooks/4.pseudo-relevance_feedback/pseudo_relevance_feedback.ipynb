{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load train_targets finish\n",
      "load test_targets finish\n",
      "merge finish\n",
      "load db finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/python/myenv/medical-coding-reproducibility-main/src/metrics.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.threshold = torch.tensor(self.threshold).clone().to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load logits finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/python/myenv/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import src.metrics as metrics\n",
    "from src.utils.fun_retrieval import pseudo_relevance_feedback\n",
    "from src.utils.seed import set_seed\n",
    "import csv\n",
    "set_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Load the .feather file into a Pandas DataFrame\n",
    "from src.settings import best_runs\n",
    "\n",
    "start=0\n",
    "stop=20000\n",
    "for model,run in best_runs.items():\n",
    "\n",
    "    train_targets = pd.read_feather(best_runs[model] + '/train_targets.feather')\n",
    "    train_targets = torch.tensor(train_targets.values, dtype=torch.float32, device=device)\n",
    "    print('load train_targets finish')\n",
    "\n",
    "    # val_targets = pd.read_feather(best_runs[model] + '/val_targets.feather')\n",
    "    # val_targets = torch.tensor(val_targets.values, dtype=torch.float32, device=device)\n",
    "    # print('load val_targets finish')\n",
    "\n",
    "\n",
    "    test_targets = pd.read_feather(best_runs[model] + '/test_targets.feather')\n",
    "    test_targets = torch.tensor(test_targets.values[start:stop], dtype=torch.float32, device=device)\n",
    "    print('load test_targets finish')\n",
    "\n",
    "    \n",
    "    # Merge all into retrieve\n",
    "    retrieve = torch.cat([train_targets,\n",
    "                          #val_targets,\n",
    "                          #test_targets\n",
    "                        ], dim=0)\n",
    "    del train_targets\n",
    "    #del val_targets\n",
    "    print('merge finish')\n",
    "\n",
    "    \n",
    "    loaded_tensor = torch.load(best_runs[model] + '/best_model.pt', map_location='cpu', weights_only=True) \n",
    "    db=loaded_tensor['db']\n",
    "    del loaded_tensor\n",
    "    print('load db finish')\n",
    "\n",
    "\n",
    "    number_of_classes=retrieve.shape[1]\n",
    "    \n",
    "    metric_collection = metrics.MetricCollection(\n",
    "        [   \n",
    "            metrics.AUC(number_of_classes=number_of_classes, average=\"micro\"),\n",
    "            metrics.AUC(number_of_classes=number_of_classes, average=\"macro\"),\n",
    "            metrics.F1Score(\n",
    "                number_of_classes=number_of_classes, average=\"micro\"\n",
    "            ),\n",
    "            metrics.F1Score(\n",
    "                number_of_classes=number_of_classes, average=\"macro\"\n",
    "            ),\n",
    "            metrics.ExactMatchRatio(number_of_classes=number_of_classes),\n",
    "            metrics.Precision_K(k=8, number_of_classes=number_of_classes),\n",
    "            metrics.Precision_K(k=15, number_of_classes=number_of_classes),\n",
    "            metrics.PrecisionAtRecall(),\n",
    "            metrics.MeanAveragePrecision(),\n",
    "            metrics.Precision(\n",
    "                number_of_classes=number_of_classes, average=\"micro\"\n",
    "            ),\n",
    "            metrics.Recall(\n",
    "                number_of_classes=number_of_classes, average=\"micro\"\n",
    "            ),\n",
    "            metrics.FPR(number_of_classes=test_targets.shape[1])\n",
    "        ]\n",
    "    )\n",
    "    metric_collection.set_threshold(db)\n",
    "    metric_collection.to(device=device)\n",
    "\n",
    "    \n",
    "    alpha=1\n",
    "    beta=0.1\n",
    "    gramma=0.0\n",
    "    TopKSelections=[10,15]\n",
    "    CosSim_Thresh=0.00\n",
    "    results = []\n",
    "\n",
    "\n",
    "    for TopKSelection in TopKSelections:\n",
    "        \n",
    "        predictions_test = pd.read_feather(best_runs[model] +'/predictions_test.feather').iloc[:,:-2]\n",
    "\n",
    "        # Convert the DataFrame back to a PyTorch tensor\n",
    "        predictions_test = torch.tensor(predictions_test.values[start:stop], dtype=torch.float32, device=device)\n",
    "        print('load logits finish')\n",
    "\n",
    "        batch = {\"logits\": predictions_test, \"targets\": test_targets}\n",
    "        metric_collection.update(batch)\n",
    "        result = {'model':model,'doc':f'{TopKSelection}','psr':f'{alpha}_{beta}_{gramma}','iteration':0,'TopKSelection':TopKSelection,'CosSim_Thresh':CosSim_Thresh}\n",
    "        result.update({key: round(value.item() * 100, 1) for key, value in metric_collection.compute(predictions_test, test_targets).items()})\n",
    "        results.append(result)\n",
    "        metric_collection.reset()\n",
    "\n",
    " \n",
    "        # # Convert predictions to binary tensor based on threshold 'db'\n",
    "        #predictions_test = (predictions_test > db).float()\n",
    "        #metric_collection.set_threshold(db)\n",
    "    \n",
    "\n",
    "\n",
    "        for i in range(1,11):\n",
    "            \n",
    "            Rocchio = pseudo_relevance_feedback(retrieve, predictions_test, TopKSelection=TopKSelection,consine_threshold=CosSim_Thresh,\n",
    "                                                alpha=alpha, beta=beta, gramma=gramma, \n",
    "            chunk_size_b=20000)\n",
    "        \n",
    "            predictions_test = Rocchio\n",
    "            batch = {\"logits\": predictions_test, \"targets\": test_targets}\n",
    "            metric_collection.update(batch)\n",
    "            result = {'model':model,'doc':f'{TopKSelection}','psr':f'{alpha}_{beta}_{gramma}','iteration': i,'TopKSelection':TopKSelection,'CosSim_Thresh':CosSim_Thresh}\n",
    "            result.update({key: round(value.item() * 100, 1) for key, value in metric_collection.compute(predictions_test, test_targets).items()})\n",
    "            results.append(result)\n",
    "            metric_collection.set_threshold(db)\n",
    "            metric_collection.reset()\n",
    "\n",
    "                \n",
    "\n",
    "        # Example dictionary\n",
    "\n",
    "\n",
    "        # Specify the file name\n",
    "        filename ='./files/retrieval/pseudo_relevance_feedback.csv'\n",
    "\n",
    "\n",
    "        # Determine the mode: 'a' for append, 'w' for write (create/overwrite)\n",
    "        file_exists = os.path.exists(filename)\n",
    "        mode = 'a' if file_exists else 'w'\n",
    "        \n",
    "        # Writing to or appending to CSV\n",
    "        with open(filename, mode=mode, newline=\"\") as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=results[0].keys())\n",
    "            \n",
    "            # Write header only if the file does not exist\n",
    "            if not file_exists:\n",
    "                writer.writeheader()\n",
    "            \n",
    "            # Write the data rows\n",
    "            writer.writerows(results)\n",
    "\n",
    "        print(f\"Data {model}_{TopKSelection}_{beta} has been {'appended to' if file_exists else 'written to'} {filename}.\")\n",
    "        results = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
