{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/python/myenv/medical-coding-reproducibility-main\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('../../')\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_87750/1143752983.py:49: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  initialize(config_path=\"../../configs\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">'Device: cpu'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m'Device: cpu'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">'CUDA_VISIBLE_DEVICES: [6]'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m'CUDA_VISIBLE_DEVICES: \u001b[0m\u001b[32m[\u001b[0m\u001b[32m6\u001b[0m\u001b[32m]\u001b[0m\u001b[32m'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import logging\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from hydra import initialize, compose\n",
    "from rich.pretty import pprint\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "\n",
    "from src.data.data_pipeline import data_pipeline\n",
    "from src.factories import (\n",
    "    get_callbacks,\n",
    "    get_dataloaders,\n",
    "    get_datasets,\n",
    "    get_lookups,\n",
    "    get_lr_scheduler,\n",
    "    get_metric_collections,\n",
    "    get_model,\n",
    "    get_optimizer,\n",
    "    get_text_encoder,\n",
    "    get_transform,\n",
    ")\n",
    "from src.trainer.trainer import Trainer\n",
    "from src.utils.seed import set_seed\n",
    "\n",
    "\n",
    "\n",
    "LOGGER = logging.getLogger(name='test')\n",
    "LOGGER.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "\n",
    "def deterministic() -> None:\n",
    "    \"\"\"Run experiment deterministically. There will still be some randomness in the backward pass of the model.\"\"\"\n",
    "    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"\n",
    "\n",
    "    import torch\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "\n",
    "# Clear the global Hydra instance\n",
    "GlobalHydra.instance().clear()\n",
    "#Load configuration\n",
    "\n",
    "\n",
    "initialize(config_path=\"../../configs\")\n",
    "#caml multi_res_conv vanillaconv vanillagru laat plm_icd\n",
    "cfg = compose(config_name=\"config\",\n",
    "              overrides=[\"experiment=mimiciv_icd10/plm_icd.yaml\",\n",
    "                         \"callbacks=no_wandb\",\n",
    "                         \"load_model=null\",\n",
    "                         \"trainer.epochs=0\"]\n",
    "                          )\n",
    "mapping_model = {\n",
    "    \"CAML\": \"./experiments/jkz0ogtz\",\n",
    "    \"PLMICD\": \"./experiments/jfywyfod\",\n",
    "    \"LAAT\": \"./experiments/gsycv1tg\",\n",
    "    \"MultiResCNN\": \"./experiments/t1dbhfub\",\n",
    "    \"VanillaRNN\": \"./experiments/mjukdu9c\",\n",
    "    \"VanillaConv\": \"./experiments/vy559dlj\",\n",
    "}\n",
    "\n",
    "cfg.load_model = mapping_model[cfg.model.name]\n",
    "\n",
    "if cfg.deterministic:\n",
    "    deterministic()\n",
    "else:\n",
    "    import torch\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"GPU is available\")\n",
    "        print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        print(\"GPU is not available\")\n",
    "set_seed(cfg.seed)\n",
    "\n",
    "\n",
    "# Check if CUDA_VISIBLE_DEVICES is set\n",
    "if \"CUDA_VISIBLE_DEVICES\" not in os.environ:\n",
    "    if cfg.gpu != -1 and cfg.gpu is not None and cfg.gpu != \"\":\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = (\n",
    "            \",\".join([str(gpu) for gpu in cfg.gpu])\n",
    "            if isinstance(cfg.gpu, list)\n",
    "            else str(cfg.gpu)\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pprint(f\"Device: {device}\")\n",
    "pprint(f\"CUDA_VISIBLE_DEVICES: {os.environ['CUDA_VISIBLE_DEVICES']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seed': 1337, 'deterministic': False, 'gpu': [6], 'name': None, 'debug': False, 'load_model': './experiments/jfywyfod', 'data': {'dir': 'files/data/mimiciv_icd10', 'data_filename': 'mimiciv_icd10.feather', 'split_filename': 'mimiciv_icd10_split.feather', 'code_column_names': ['icd10_diag', 'icd10_proc'], 'max_length': 4000}, 'dataset': {'name': 'HuggingfaceDataset', 'configs': {'chunk_size': 128}}, 'dataloader': {'max_batch_size': 1, 'batch_size': 16, 'num_workers': 0, 'drop_last': True, 'pin_memory': False, 'batch_sampler': {'name': 'BySequenceLengthSampler', 'configs': {'bucket_boundaries': [400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000, 2200, 2600, 3000, 4000]}}}, 'model': {'name': 'PLMICD', 'configs': {'model_path': './files/hfs/RoBERTa-base-PM-M3-Voc-hf'}}, 'text_encoder': {}, 'trainer': {'epochs': 0, 'validate_on_training_data': True, 'print_metrics': False, 'use_amp': True, 'threshold_tuning': True}, 'optimizer': {'name': 'AdamW', 'configs': {'lr': 5e-05, 'weight_decay': 0}}, 'lr_scheduler': {'name': 'linear', 'configs': {'num_warmup_steps': 2000}}, 'label_transform': {'name': 'OneHotEncoder', 'configs': {}}, 'text_transform': {'name': 'HuggingFaceTokenizer', 'configs': {'model_path': './files/hfs/RoBERTa-base-PM-M3-Voc-hf', 'add_special_token': True, 'padding': False, 'use_fast': True, 'do_lower_case': True, 'max_length': None, 'truncation': False}}, 'metrics': [{'name': 'F1Score', 'configs': {'average': 'micro', 'threshold': 0.5}}, {'name': 'F1Score', 'configs': {'average': 'macro', 'threshold': 0.5}}, {'name': 'Recall', 'configs': {'average': 'micro', 'threshold': 0.5}}, {'name': 'Recall', 'configs': {'average': 'macro', 'threshold': 0.5}}, {'name': 'Precision', 'configs': {'average': 'micro', 'threshold': 0.5}}, {'name': 'Precision', 'configs': {'average': 'macro', 'threshold': 0.5}}, {'name': 'FPR', 'configs': {'average': 'micro', 'threshold': 0.5}}, {'name': 'FPR', 'configs': {'average': 'macro', 'threshold': 0.5}}, {'name': 'ExactMatchRatio', 'configs': {'threshold': 0.5}}, {'name': 'Precision_K', 'configs': {'k': 1}}, {'name': 'Precision_K', 'configs': {'k': 5}}, {'name': 'Precision_K', 'configs': {'k': 8}}, {'name': 'Precision_K', 'configs': {'k': 15}}, {'name': 'Recall_K', 'configs': {'k': 10}}, {'name': 'Recall_K', 'configs': {'k': 15}}, {'name': 'MeanAveragePrecision', 'configs': {}}, {'name': 'PrecisionAtRecall', 'configs': {}}, {'name': 'AUC', 'configs': {'average': 'micro'}}, {'name': 'AUC', 'configs': {'average': 'macro'}}, {'name': 'F1Score', 'configs': {'average': 'micro', 'threshold': 0.5, 'filter_codes': False}}, {'name': 'F1Score', 'configs': {'average': 'macro', 'threshold': 0.5, 'filter_codes': False}}, {'name': 'Recall', 'configs': {'average': 'micro', 'threshold': 0.5, 'filter_codes': False}}, {'name': 'Recall', 'configs': {'average': 'macro', 'threshold': 0.5, 'filter_codes': False}}, {'name': 'Precision', 'configs': {'average': 'micro', 'threshold': 0.5, 'filter_codes': False}}, {'name': 'Precision', 'configs': {'average': 'macro', 'threshold': 0.5, 'filter_codes': False}}, {'name': 'FPR', 'configs': {'average': 'micro', 'threshold': 0.5, 'filter_codes': False}}, {'name': 'FPR', 'configs': {'average': 'macro', 'threshold': 0.5, 'filter_codes': False}}, {'name': 'ExactMatchRatio', 'configs': {'threshold': 0.5, 'filter_codes': False}}, {'name': 'Precision_K', 'configs': {'k': 1, 'filter_codes': False}}, {'name': 'Precision_K', 'configs': {'k': 5, 'filter_codes': False}}, {'name': 'Precision_K', 'configs': {'k': 8, 'filter_codes': False}}, {'name': 'Precision_K', 'configs': {'k': 15, 'filter_codes': False}}, {'name': 'Recall_K', 'configs': {'k': 10, 'filter_codes': False}}, {'name': 'Recall_K', 'configs': {'k': 15, 'filter_codes': False}}, {'name': 'MeanAveragePrecision', 'configs': {'filter_codes': False}}, {'name': 'PrecisionAtRecall', 'configs': {'filter_codes': False}}, {'name': 'AUC', 'configs': {'average': 'micro', 'filter_codes': False}}, {'name': 'LossMetric', 'configs': {}}], 'lookup': {'code_description': {'code_desc_path': None, 'code_column': None, 'desc_column': None}}, 'callbacks': [{'name': 'SaveBestModelCallback', 'configs': {'split': 'val', 'target': 'all', 'metric': 'precision@8'}}, {'name': 'EarlyStoppingCallback', 'configs': {'split': 'val', 'target': 'all', 'metric': 'precision@8', 'patience': 10}}], 'data.max_length': 4000, 'tune': {'NavgTop': 10, 'w_beta': 0.1}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "พูดถึง vaex\n",
    "\n",
    "'split_filename': 'mimiciv_icd10_split.feather'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_pipeline(config=cfg.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = get_text_encoder(\n",
    "    config=cfg.text_encoder, data_dir=cfg.data.dir, texts=data.get_train_documents\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ตรงนี้เปลี่ยน target เป็น int นะ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded transform\n"
     ]
    }
   ],
   "source": [
    "label_transform = get_transform(\n",
    "    config=cfg.label_transform,\n",
    "    targets=data.all_targets,\n",
    "    load_transform_path=cfg.load_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set TokenSequence, HuggingFaceTokenizer, BOW แต่งเติมส่วนของ padding unknow ให้เรียบร้อย"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./files/hfs/RoBERTa-base-PM-M3-Voc-hf\n",
      "loaded transform\n"
     ]
    }
   ],
   "source": [
    "text_transform = get_transform(\n",
    "    config=cfg.text_transform,\n",
    "    texts=data.get_train_documents,\n",
    "    text_encoder=text_encoder,\n",
    "    load_transform_path=cfg.load_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tranform text จริง โดยมองเป็น batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.truncate_text(cfg.data.max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transforming text...: 100%|██████████| 31/31 [00:02<00:00, 14.49it/s]\n",
      "Collecting results...: 100%|██████████| 31/31 [00:00<00:00, 1725.68it/s]\n"
     ]
    }
   ],
   "source": [
    "#transform token=>index ตัวเลข เด๋ว train จะถูกผลักออกเป็น vector \n",
    "data.transform_text(text_transform.batch_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target มีทำ map กลับเป็น int ไว้เลย"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'average_classes_per_example': 15.65479481182224,\n",
      " 'average_tokens_per_example': 1596.879217847855,\n",
      " 'num_classes': 7942,\n",
      " 'num_examples': 122278,\n",
      " 'num_test_classes': 7937,\n",
      " 'num_test_examples': 21265,\n",
      " 'num_train_classes': 7938,\n",
      " 'num_train_examples': 76933,\n",
      " 'num_train_tokens': 122848593,\n",
      " 'num_val_classes': 7932,\n",
      " 'num_val_examples': 24080,\n",
      " 'pad_index': 0,\n",
      " 'vocab_size': 50001}\n"
     ]
    }
   ],
   "source": [
    "lookups = get_lookups(\n",
    "    config=cfg.lookup,\n",
    "    data=data,\n",
    "    label_transform=label_transform,\n",
    "    text_transform=text_transform,\n",
    ")\n",
    "\n",
    "# print data info\n",
    "pprint(lookups.data_info)\n",
    "# pprint(lookups.data_info[\"num_classes\"] == len(lookups.code_system2code_indices['icd10_diag'])+len(lookups.code_system2code_indices['icd10_proc']))\n",
    "\n",
    "\n",
    "#สังเกตแค่่ train เท่านั้นหากเราพยายามเปลียน input เพื่อทดสอบ n น้อยๆ ให้ run model ผ่าน ตอน val / test ถูกล็อคไว้หมดแล้ว"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ระบุโมเดล"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PLMICD(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50008, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (attention): LabelAttention(\n",
       "    (first_linear): Linear(in_features=768, out_features=768, bias=False)\n",
       "    (second_linear): Linear(in_features=768, out_features=7942, bias=False)\n",
       "    (third_linear): Linear(in_features=768, out_features=7942, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_model(\n",
    "    config=cfg.model, data_info=lookups.data_info, text_encoder=text_encoder\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 137244934\n"
     ]
    }
   ],
   "source": [
    "# นับจำนวนพารามิเตอร์ทั้งหมด\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# ตรวจสอบจำนวนพารามิเตอร์ในโมเดล\n",
    "num_parameters = count_parameters(model)\n",
    "print(f\"Total trainable parameters: {num_parameters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "จัดเตรียม datasset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = get_datasets(\n",
    "    config=cfg.dataset,\n",
    "    data=data,\n",
    "    text_transform=text_transform,\n",
    "    label_transform=label_transform,\n",
    "    lookups=lookups,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "สามารถปรับ batch Optimize ได้ตรงนี้เลย ตรงนี้เป็นการบังคับเฉพาะให้เฉพาะ train เท่านั้น ถูกควบคุมโดย cfg.dataloader.name\n",
    "\n",
    "set loader มีทั้ง train train_val test val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ตรงนี้เตรียม batch ให้เรียบร้อยแล้วหละ เดิม #dataset ยังไม่ทำเป็น batch นะ \n",
    "dataloaders = get_dataloaders(config=cfg.dataloader, datasets_dict=datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = get_optimizer(config=cfg.optimizer, model=model)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "เรื่องของ batchsize ตำนวนหา gradient กรณี accumulate_grad_batches > 1 แสดงว่ายังไม่ปรับ grad ทันที & Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulate_grad_batches = int(\n",
    "    max(cfg.dataloader.batch_size / cfg.dataloader.max_batch_size, 1)\n",
    ")\n",
    "num_training_steps = (\n",
    "    math.ceil(len(dataloaders[\"train\"]) / accumulate_grad_batches)\n",
    "    * cfg.trainer.epochs\n",
    ")\n",
    "lr_scheduler = get_lr_scheduler(\n",
    "    config=cfg.lr_scheduler,\n",
    "    optimizer=optimizer,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split_names: list[str] = [\"train\", \"train_val\", \"val\", \"test\"],\n",
    "\n",
    "splits_with_multiple_code_systems: set[str] = {\"train_val\", \"val\", \"test\"},"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code_system2code_indices => diag proc (ทั้งหมดเลยนะไม่แยก train test)\n",
    "#split2code_indices =>  train train_val val test\n",
    "metric_collections = get_metric_collections(\n",
    "    config=cfg.metrics,\n",
    "    number_of_classes=lookups.data_info[\"num_classes\"],\n",
    "    code_system2code_indices=lookups.code_system2code_indices, # รวมทั้งหมดที่เป็น label diag และ label proc\n",
    "    split2code_indices=lookups.split2code_indices, # label classs แต่ละ กกลุ่มที่แบ่งไป ตาม \n",
    ")\n",
    "metric_collections\n",
    "\n",
    "#ข้างในมี แต่ index ล้วนๆเลยนะ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- name: WandbCallback\n",
    "\n",
    "- name: SaveBestModelCallback\n",
    "\n",
    "- name: EarlyStoppingCallback\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = get_callbacks(config=cfg.callbacks)\n",
    "callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval = Trainer(\n",
    "    config=cfg,\n",
    "    data=data,\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    dataloaders=dataloaders,\n",
    "    metric_collections=metric_collections, # metric มี 4 กลุ่ม ตอน ใช้ metric จะแยกกล่มใครกล่มมั่นไปแล้ว\n",
    "    callbacks=callbacks,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    lookups=lookups,\n",
    "    accumulate_grad_batches=accumulate_grad_batches,\n",
    "    experiment_path = Path(cfg.load_model) # ไว้สำหรับ eval \n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eval output จะเก็บไว้ที่ load_model เลย"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.load_model:\n",
    "    eval.experiment_path = Path(cfg.load_model)\n",
    "eval.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
