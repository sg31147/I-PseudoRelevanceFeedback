{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/python/myenv/medical-coding-reproducibility-main\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('../../')\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_runs = {\n",
    "                \"CAML\": \"./experiments/jkz0ogtz\",\n",
    "                \"PLMICD\": \"./experiments/jfywyfod\",\n",
    "                \"LAAT\": \"./experiments/gsycv1tg\",\n",
    "                \"MultiResCNN\": \"./experiments/t1dbhfub\",\n",
    "                \"VanillaRNN\": \"./experiments/mjukdu9c\",\n",
    "                \"VanillaConv\": \"./experiments/vy559dlj\",\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import src.metrics as metrics\n",
    "from src.utils.fun_retrieval import prf\n",
    "from src.utils.seed import set_seed\n",
    "import csv\n",
    "set_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Load the .feather file into a Pandas DataFrame\n",
    "for model,run in best_runs.items():\n",
    "\n",
    "    targets = pd.read_feather(best_runs[model] + '/test_targets.feather')\n",
    "    print('load target finish')\n",
    "    # Convert the DataFrame back to a PyTorch tensor\n",
    "    targets = torch.tensor(targets.values).to(device)\n",
    "  \n",
    "\n",
    "    retrieve = pd.read_feather(best_runs[model] + '/retrieve.feather')\n",
    "    retrieve = torch.tensor(retrieve.values).to(device)\n",
    "    print('load retrieve finish')\n",
    "\n",
    "\n",
    "    # Load the tensor back\n",
    "    loaded_tensor = torch.load(best_runs[model] + '/best_model.pt', map_location='cpu', weights_only=True)\n",
    "    \n",
    "    db=loaded_tensor['db']\n",
    "    del loaded_tensor\n",
    "    print('load db finish')\n",
    "\n",
    "\n",
    "    number_of_classes=retrieve.shape[1]\n",
    "    metric_collection = metrics.MetricCollection(\n",
    "        [   \n",
    "            metrics.AUC(number_of_classes=number_of_classes, average=\"micro\"),\n",
    "            metrics.AUC(number_of_classes=number_of_classes, average=\"macro\"),\n",
    "            metrics.F1Score(\n",
    "                number_of_classes=number_of_classes, average=\"micro\", threshold=db\n",
    "            ),\n",
    "            metrics.F1Score(\n",
    "                number_of_classes=number_of_classes, average=\"macro\", threshold=db\n",
    "            ),\n",
    "            metrics.ExactMatchRatio(number_of_classes=number_of_classes, threshold=db),\n",
    "            metrics.Precision_K(k=8, number_of_classes=number_of_classes),\n",
    "            metrics.Precision_K(k=15, number_of_classes=number_of_classes),\n",
    "            metrics.MeanAveragePrecision(),\n",
    "            metrics.PrecisionAtRecall(),\n",
    "            metrics.Precision(\n",
    "                number_of_classes=number_of_classes, average=\"micro\", threshold=db\n",
    "            ),\n",
    "            metrics.Recall(\n",
    "                number_of_classes=number_of_classes, average=\"micro\", threshold=db\n",
    "            ),\n",
    "            metrics.FPR(number_of_classes=targets.shape[1], threshold=db)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    w_alpha=1\n",
    "    w_beta=0.1\n",
    "    w_gramma=0.1\n",
    "    AvgTopRs=[5,10,20,40]\n",
    "    AvgLowR=10\n",
    "    results = []\n",
    "\n",
    "    for AvgTopR in AvgTopRs:\n",
    "        \n",
    "        logits = pd.read_feather(best_runs[model] +'/predictions_test.feather').iloc[:,:-2]\n",
    "        # Convert the DataFrame back to a PyTorch tensor\n",
    "        logits = torch.tensor(logits.values).to(device)\n",
    "        print('load logits finish')\n",
    "\n",
    "        batch = {\"logits\": logits, \"targets\": targets}\n",
    "        metric_collection.update(batch)\n",
    "        result = {'model_NavgTop_w_beta':f'{model}_{AvgTopR}_{w_beta}','iteration': 0,'model':model,'NavgTop':AvgTopR,'w_beta':w_beta}\n",
    "        result.update({key: round(value.item() * 100, 1) for key, value in metric_collection.compute(logits, targets).items()})\n",
    "        results.append(result)\n",
    "        metric_collection.reset()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        for i in range(1,11):\n",
    "\n",
    "            Rocchio = prf(retrieve, logits, AvgTopR=AvgTopR, AvgLowR=AvgLowR, \n",
    "                                               w_alpha=w_alpha, w_beta=w_beta, w_gramma=w_gramma, \n",
    "                                               chunk_size_b=80000)\n",
    "            logits = Rocchio\n",
    "            batch = {\"logits\": logits, \"targets\": targets}\n",
    "            metric_collection.update(batch)\n",
    "            result = {'model_NavgTop_w_beta':f'{model}_{AvgTopR}_{w_beta}','iteration': i,'model':model,'NavgTop':AvgTopR,'w_beta':w_beta}\n",
    "            result.update({key: round(value.item() * 100, 1) for key, value in metric_collection.compute(logits, targets).items()})\n",
    "            results.append(result)\n",
    "\n",
    "            metric_collection.reset()\n",
    "\n",
    "                \n",
    "\n",
    "        # Example dictionary\n",
    "\n",
    "\n",
    "        # Specify the file name\n",
    "        filename ='./files/retrieval/retrieval.csv'\n",
    "\n",
    "\n",
    "        # Determine the mode: 'a' for append, 'w' for write (create/overwrite)\n",
    "        file_exists = os.path.exists(filename)\n",
    "        mode = 'a' if file_exists else 'w'\n",
    "\n",
    "        # Writing to or appending to CSV\n",
    "        with open(filename, mode=mode, newline=\"\") as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=results[0].keys())\n",
    "            \n",
    "            # Write header only if the file does not exist\n",
    "            if not file_exists:\n",
    "                writer.writeheader()\n",
    "            \n",
    "            # Write the data rows\n",
    "            writer.writerows(results)\n",
    "\n",
    "        print(f\"Data {model}_{AvgTopR}_{w_beta} has been {'appended to' if file_exists else 'written to'} {filename}.\")\n",
    "        results = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
