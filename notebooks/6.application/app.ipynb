{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/python/myenv/medical-coding-reproducibility-main\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('../../')\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9670/1880661831.py:65: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  initialize(config_path=\"../../configs\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json \n",
    "from hydra import initialize, compose\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "import nest_asyncio\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel,Field\n",
    "from uvicorn import Config, Server\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "import gc\n",
    "from src.utils.fun_retrieval import prf\n",
    "\n",
    "from src.settings import (\n",
    "    DOWNLOAD_DIRECTORY_MIMICIV\n",
    ")\n",
    "\n",
    "\n",
    "from prepare_data.utils import (\n",
    "    TextPreprocessor,\n",
    "    preprocess_documents,\n",
    "    load_gz_file_into_df,\n",
    "    ID_COLUMN, SUBJECT_ID_COLUMN, TARGET_COLUMN, TEXT_COLUMN\n",
    "    \n",
    ")\n",
    "from src.data.data_pipeline import data_predict_pipeline\n",
    "from src.factories import (\n",
    "    get_callbacks,\n",
    "    get_dataloaders,\n",
    "    get_datasets,\n",
    "    get_lookups,\n",
    "    get_lr_scheduler,\n",
    "    get_metric_collections,\n",
    "    get_model,\n",
    "    get_optimizer,\n",
    "    get_text_encoder,\n",
    "    get_transform,\n",
    ")\n",
    "from src.trainer.trainer import Trainer\n",
    "from src.utils.seed import set_seed\n",
    "\n",
    "LOGGER = logging.getLogger(name='test')\n",
    "LOGGER.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "\n",
    "def deterministic() -> None:\n",
    "    \"\"\"Run experiment deterministically. There will still be some randomness in the backward pass of the model.\"\"\"\n",
    "    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"\n",
    "\n",
    "    import torch\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "\n",
    "# Clear the global Hydra instance\n",
    "GlobalHydra.instance().clear()\n",
    "#Load configuration\n",
    "\n",
    "initialize(config_path=\"../../configs\")\n",
    "#caml multi_res_conv vanillaconv vanillagru laat plm_icd\n",
    "cfg = compose(config_name=\"config\",\n",
    "              overrides=[\"experiment=mimiciv_icd10/plm_icd.yaml\",\n",
    "                         \"callbacks=no_wandb\",\n",
    "                         \"load_model=null\",\"trainer.epochs=0\"]\n",
    "                        )\n",
    "\n",
    "best_runs = {\n",
    "            \"CAML\": \"./experiments/jkz0ogtz\",\n",
    "            \"PLMICD\": \"./experiments/jfywyfod\",\n",
    "            \"LAAT\": \"./experiments/gsycv1tg\",\n",
    "            \"MultiResCNN\": \"./experiments/t1dbhfub\",\n",
    "            \"VanillaRNN\": \"./experiments/mjukdu9c\",\n",
    "            \"VanillaConv\": \"./experiments/vy559dlj\",\n",
    "            }\n",
    "\n",
    "\n",
    "cfg.load_model = best_runs[cfg.model.name]\n",
    "\n",
    "\n",
    "if cfg.deterministic:\n",
    "    deterministic()\n",
    "else:\n",
    "    import torch\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"GPU is available\")\n",
    "        print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        print(\"GPU is not available\")\n",
    "set_seed(cfg.seed)\n",
    "\n",
    "\n",
    "# Check if CUDA_VISIBLE_DEVICES is set\n",
    "if \"CUDA_VISIBLE_DEVICES\" not in os.environ:\n",
    "    if cfg.gpu != -1 and cfg.gpu is not None and cfg.gpu != \"\":\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = (\n",
    "            \",\".join([str(gpu) for gpu in cfg.gpu])\n",
    "            if isinstance(cfg.gpu, list)\n",
    "            else str(cfg.gpu)\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "torch.set_num_threads(os.cpu_count())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping Definition\n",
    "download_dir = Path(DOWNLOAD_DIRECTORY_MIMICIV)\n",
    "\n",
    "d_icd_procedures = load_gz_file_into_df(\n",
    "    download_dir / \"hosp/d_icd_procedures.csv.gz\", dtype={\"icd_code\": str}\n",
    ")\n",
    "d_icd_procedures=d_icd_procedures[d_icd_procedures['icd_version']==10]\n",
    "d_icd_diagnoses = load_gz_file_into_df(\n",
    "    download_dir / \"hosp/d_icd_diagnoses.csv.gz\", dtype={\"icd_code\": str}\n",
    ")\n",
    "d_icd_diagnoses=d_icd_diagnoses[d_icd_diagnoses['icd_version']==10]\n",
    "d_icd_diagnoses = d_icd_diagnoses[['icd_code', 'long_title']].set_index('icd_code')['long_title'].to_dict()\n",
    "d_icd_procedures = d_icd_procedures[['icd_code', 'long_title']].set_index('icd_code')['long_title'].to_dict()\n",
    "d_icd={**d_icd_diagnoses,**d_icd_procedures}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load retrieve finish\n"
     ]
    }
   ],
   "source": [
    "#retrieval\n",
    "retrieve = pd.read_feather(best_runs[cfg.model.name] + '/retrieve.feather')\n",
    "retrieve = torch.tensor(retrieve.values).to(device)\n",
    "print('load retrieve finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your JSON file\n",
    "file_path = best_runs[cfg.model.name] +'/target2index.json'\n",
    "\n",
    "# Open the file and load the JSON data\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "token2index = {token: index for index, token in data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [9670]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8081 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     115.87.76.57:62358 - \"POST /predict HTTP/1.1\" 307 Temporary Redirect\n",
      "code_system2code_counts already exists\n",
      "loaded transform\n",
      "./files/hfs/RoBERTa-base-PM-M3-Voc-hf\n",
      "loaded transform\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transforming text...: 100%|██████████| 1/1 [00:00<00:00, 2853.27it/s]\n",
      "Collecting results...: 100%|██████████| 1/1 [00:00<00:00, 15827.56it/s]\n",
      "Creating examples train: 0it [00:00, ?it/s]\n",
      "Creating examples val: 0it [00:00, ?it/s]\n",
      "Creating examples test: 100%|██████████| 2/2 [00:00<00:00, 636.46it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">'Train batch size: 1'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m'Train batch size: 1'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: drop_last=True, dropping last non batch-size batch in every bucket ... \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">'Accumulating gradients over 16 batch(es).'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m'Accumulating gradients over 16 batch\u001b[0m\u001b[32m(\u001b[0m\u001b[32mes\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/python/myenv/medical-coding-reproducibility-main/src/trainer/trainer.py:56: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.gradient_scaler = torch.cuda.amp.GradScaler(enabled=self.use_amp)\n",
      "/root/python/myenv/lib/python3.10/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PosixPath</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'experiments/jfywyfod'</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mPosixPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'experiments/jfywyfod'\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/python/myenv/medical-coding-reproducibility-main/src/trainer/trainer.py:423: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(self.experiment_path / file_name,map_location=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">'Loaded checkpoint from experiments/jfywyfod/best_model.pt'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m'Loaded checkpoint from experiments/jfywyfod/best_model.pt'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 19 | Validating on test:   0%|          | 0/2 [00:00<?, ?it/s]/root/python/myenv/lib/python3.10/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "Epoch: 19 | Validating on test: 100%|██████████| 2/2 [00:01<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     115.87.76.57:62358 - \"POST /predict/ HTTP/1.1\" 200 OK\n",
      "INFO:     184.105.139.81:57407 - \"GET / HTTP/1.1\" 404 Not Found\n",
      "INFO:     127.0.0.1:32856 - \"GET /mapping HTTP/1.1\" 307 Temporary Redirect\n",
      "INFO:     127.0.0.1:32856 - \"GET /mapping/ HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:59974 - \"GET /mapping HTTP/1.1\" 307 Temporary Redirect\n",
      "INFO:     127.0.0.1:59974 - \"GET /mapping/ HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:59980 - \"GET /mapping HTTP/1.1\" 307 Temporary Redirect\n",
      "INFO:     127.0.0.1:59980 - \"GET /mapping/ HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:59982 - \"POST /predict HTTP/1.1\" 307 Temporary Redirect\n",
      "code_system2code_counts already exists\n",
      "loaded transform\n",
      "./files/hfs/RoBERTa-base-PM-M3-Voc-hf\n",
      "loaded transform\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transforming text...: 100%|██████████| 1/1 [00:00<00:00, 2962.08it/s]\n",
      "Collecting results...: 100%|██████████| 1/1 [00:00<00:00, 16844.59it/s]\n",
      "Creating examples train: 0it [00:00, ?it/s]\n",
      "Creating examples val: 0it [00:00, ?it/s]\n",
      "Creating examples test: 100%|██████████| 1/1 [00:00<00:00, 2159.79it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">'Train batch size: 1'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m'Train batch size: 1'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: drop_last=True, dropping last non batch-size batch in every bucket ... \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">'Accumulating gradients over 16 batch(es).'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m'Accumulating gradients over 16 batch\u001b[0m\u001b[32m(\u001b[0m\u001b[32mes\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PosixPath</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'experiments/jfywyfod'</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mPosixPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'experiments/jfywyfod'\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">'Loaded checkpoint from experiments/jfywyfod/best_model.pt'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m'Loaded checkpoint from experiments/jfywyfod/best_model.pt'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 19 | Validating on test: 100%|██████████| 1/1 [00:00<00:00,  3.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:59982 - \"POST /predict/ HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:42274 - \"GET /mapping HTTP/1.1\" 307 Temporary Redirect\n",
      "INFO:     127.0.0.1:42274 - \"GET /mapping/ HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:42280 - \"GET /mapping HTTP/1.1\" 307 Temporary Redirect\n",
      "INFO:     127.0.0.1:42280 - \"GET /mapping/ HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:42284 - \"POST /predict HTTP/1.1\" 307 Temporary Redirect\n",
      "code_system2code_counts already exists\n",
      "loaded transform\n",
      "./files/hfs/RoBERTa-base-PM-M3-Voc-hf\n",
      "loaded transform\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transforming text...: 100%|██████████| 1/1 [00:00<00:00, 2734.23it/s]\n",
      "Collecting results...: 100%|██████████| 1/1 [00:00<00:00, 15534.46it/s]\n",
      "Creating examples train: 0it [00:00, ?it/s]\n",
      "Creating examples val: 0it [00:00, ?it/s]\n",
      "Creating examples test: 100%|██████████| 1/1 [00:00<00:00, 1609.48it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">'Train batch size: 1'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m'Train batch size: 1'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: drop_last=True, dropping last non batch-size batch in every bucket ... \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">'Accumulating gradients over 16 batch(es).'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m'Accumulating gradients over 16 batch\u001b[0m\u001b[32m(\u001b[0m\u001b[32mes\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PosixPath</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'experiments/jfywyfod'</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mPosixPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'experiments/jfywyfod'\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">'Loaded checkpoint from experiments/jfywyfod/best_model.pt'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m'Loaded checkpoint from experiments/jfywyfod/best_model.pt'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 19 | Validating on test: 100%|██████████| 1/1 [00:00<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:42284 - \"POST /predict/ HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "# Define a Pydantic model for the data structure\n",
    "\n",
    "class TestDataModel(BaseModel):\n",
    "    id: List[int]\n",
    "    text: List[str]\n",
    "    target: List[List[str]]\n",
    "    split: str\n",
    "    Task:str\n",
    "    iteration: int = Field(2, ge=0, le=10, description=\"iteration must be between 0 and 10\") \n",
    "    w_alpha: float = Field(1, ge=0, le=1, description=\"w_alpha must be between 0 and 1\")\n",
    "    w_beta: float = Field(0.1, ge=0, le=1, description=\"w_beta must be between 0 and 1\")\n",
    "    AvgTopR: int = Field(10, ge=1, le=50, description=\"AvgTopR must be between 1 and 50\")\n",
    "    w_gramma: float = Field(0.1, ge=0, le=1, description=\"w_gramma must be between 0 and 1\")\n",
    "    AvgLowR: int = Field(10, ge=1, le=50, description=\"AvgLowR must be between 1 and 50\")\n",
    "    Precisionk: int = Field(5, ge=1, le=15, description=\"precision@k must be between 1 and 15\")\n",
    "\n",
    "# Apply nest_asyncio to allow FastAPI to run within Jupyter Notebook\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Define the FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/mapping/\")\n",
    "async def mapping():\n",
    "    return {\n",
    "        \"status\": \"load mapping successfully\",\n",
    "        \"data\": {\n",
    "            values: d_icd[values.replace(\".\", \"\")] \n",
    "                           for values in token2index.values() \n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "# Create a POST route for receiving and validating the data structure\n",
    "@app.post(\"/predict/\")\n",
    "async def predict(Testdata: TestDataModel):\n",
    "    \n",
    "    data = Testdata.model_dump(by_alias=True)\n",
    "    data = pd.DataFrame(data).rename(columns={'id': ID_COLUMN})\n",
    "    \n",
    "    #preprocesss\n",
    "    preprocessor = TextPreprocessor(\n",
    "                lower=True,\n",
    "                remove_special_characters_mullenbach=True,\n",
    "                remove_special_characters=False,\n",
    "                remove_digits=True,\n",
    "                remove_accents=False,\n",
    "                remove_brackets=False,\n",
    "                convert_danish_characters=False,\n",
    "            )\n",
    "\n",
    "    data=preprocess_documents(df=data, preprocessor=preprocessor)\n",
    "    \n",
    "\n",
    "    \n",
    "    # Here, the validated data can be processed as needed\n",
    "    data = data_predict_pipeline(config=cfg.data,data=data)\n",
    "\n",
    "    text_encoder = get_text_encoder(\n",
    "        config=cfg.text_encoder, data_dir=cfg.data.dir, texts=data.get_train_documents\n",
    "    )\n",
    "    label_transform = get_transform(\n",
    "        config=cfg.label_transform,\n",
    "        targets=data.all_targets,\n",
    "        load_transform_path=cfg.load_model,\n",
    "    )\n",
    "    text_transform = get_transform(\n",
    "        config=cfg.text_transform,\n",
    "        texts=data.get_train_documents,\n",
    "        text_encoder=text_encoder,\n",
    "        load_transform_path=cfg.load_model,\n",
    "    )\n",
    "    \n",
    "    data.truncate_text(cfg.data.max_length)\n",
    "    data.transform_text(text_transform.batch_transform)\n",
    "    lookups = get_lookups(\n",
    "        config=cfg.lookup,\n",
    "        data=data,\n",
    "        label_transform=label_transform,\n",
    "        text_transform=text_transform,\n",
    "    )\n",
    "   \n",
    "    model = get_model(\n",
    "        config=cfg.model, data_info=lookups.data_info, text_encoder=text_encoder\n",
    "    ).to(device)\n",
    "    \n",
    "    datasets = get_datasets(\n",
    "        config=cfg.dataset,\n",
    "        data=data,\n",
    "        text_transform=text_transform,\n",
    "        label_transform=label_transform,\n",
    "        lookups=lookups,\n",
    "    )\n",
    "    \n",
    "    dataloaders = get_dataloaders(config=cfg.dataloader, datasets_dict=datasets)\n",
    "    \n",
    "    optimizer = get_optimizer(config=cfg.optimizer, model=model)\n",
    "    accumulate_grad_batches = int(\n",
    "        max(cfg.dataloader.batch_size / cfg.dataloader.max_batch_size, 1)\n",
    "    )\n",
    "    \n",
    "    num_training_steps = (\n",
    "        math.ceil(len(dataloaders[\"train\"]) / accumulate_grad_batches)\n",
    "        * cfg.trainer.epochs\n",
    "    )\n",
    "    \n",
    "    lr_scheduler = get_lr_scheduler(\n",
    "        config=cfg.lr_scheduler,\n",
    "        optimizer=optimizer,\n",
    "        num_training_steps=num_training_steps,\n",
    "    )\n",
    "    metric_collections = get_metric_collections(\n",
    "        config=cfg.metrics,\n",
    "        number_of_classes=lookups.data_info[\"num_classes\"],\n",
    "        code_system2code_indices=lookups.code_system2code_indices, # รวมทั้งหมดที่เป็น label diag และ label proc\n",
    "        split2code_indices=lookups.split2code_indices, # label classs แต่ละ กกลุ่มที่แบ่งไป ตาม \n",
    "    )\n",
    "    callbacks = get_callbacks(config=cfg.callbacks)\n",
    "\n",
    "\n",
    "    pred = Trainer(\n",
    "        config=cfg,\n",
    "        data=data,\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        dataloaders=dataloaders,\n",
    "        metric_collections=metric_collections, # metric มี 4 กลุ่ม ตอน ใช้ metric จะแยกกล่มใครกล่มมั่นไปแล้ว\n",
    "        callbacks=callbacks,\n",
    "        lr_scheduler=lr_scheduler,\n",
    "        lookups=lookups,\n",
    "        accumulate_grad_batches=accumulate_grad_batches,\n",
    "        experiment_path = Path(cfg.load_model) # ไว้สำหรับ eval \n",
    "    ).to(device)\n",
    "\n",
    "    if cfg.load_model:\n",
    "        pred.experiment_path = Path(cfg.load_model)\n",
    "    predict=pred.fit(predict=True)\n",
    "    \n",
    "    ids, logits, targets = predict[\"ids\"], predict[\"logits\"], predict[\"targets\"]\n",
    "\n",
    "\n",
    "\n",
    "    Task=Testdata.Task\n",
    "    w_alpha=Testdata.w_alpha\n",
    "    w_beta=Testdata.w_beta\n",
    "    w_gramma=Testdata.w_gramma\n",
    "    AvgTopR=Testdata.AvgTopR\n",
    "    AvgLowR=Testdata.AvgLowR\n",
    "    Precisionk = Testdata.Precisionk\n",
    "    for i in range(1,Testdata.iteration):\n",
    "\n",
    "        logits=prf(retrieve, logits, AvgTopR=AvgTopR, AvgLowR=AvgLowR, \n",
    "                                                w_alpha=w_alpha, w_beta=w_beta, w_gramma=w_gramma, \n",
    "                                                chunk_size_b=80000)\n",
    " \n",
    "\n",
    "  \n",
    "\n",
    "    predict = {id_: {\"logits\": logit, \"targets\": target} for id_, logit, target in zip(ids, logits, targets)}\n",
    "\n",
    "    if Task == \"Ranking\":\n",
    "        result = {\n",
    "            ids.item(): {\n",
    "                \"id\": ids.item(),\n",
    "                \"predictions_topk\": {\n",
    "                    token2index[idx]: f'{d_icd[token2index[idx].replace(\".\", \"\")]} ({prob})'\n",
    "\n",
    "                    for idx, prob in zip(\n",
    "                        logits[\"logits\"].topk(Precisionk).indices.tolist(),\n",
    "                        logits[\"logits\"].topk(Precisionk).values.tolist()\n",
    "                    )\n",
    "                },\n",
    "                #\"target\": [token2index[idx] for idx in (data_entry[\"target\"] == 1).nonzero(as_tuple=True)[0].tolist()],\n",
    "                \"match_percentage_topk\": round(\n",
    "                sum(1 for idx in logits[\"logits\"].topk(Precisionk).indices.tolist() if token2index[idx] in [\n",
    "                    token2index[idx] for idx in (logits[\"targets\"] == 1).nonzero(as_tuple=True)[0].tolist()\n",
    "                ]) / Precisionk * 100, 2),\n",
    "            }\n",
    "            for ids, logits in predict.items()\n",
    "        }\n",
    "    else:\n",
    "        result = {\n",
    "            ids.item(): {\n",
    "                \"id\": ids.item(),\n",
    "                \"classification\": {\n",
    "                    token2index[idx]: f'{d_icd[token2index[idx].replace(\".\", \"\")]} ({prob})'\n",
    "                    for idx, prob in zip(\n",
    "                        (logits[\"logits\"] > pred.best_db).nonzero(as_tuple=True)[0].tolist(),\n",
    "                        logits[\"logits\"][logits[\"logits\"] > pred.best_db].tolist()\n",
    "                    )\n",
    "                },\n",
    "                #\"target\": [token2index[idx] for idx in (data_entry[\"target\"] == 1).nonzero(as_tuple=True)[0].tolist()],\n",
    "                \"match_percentage_classification\": round(\n",
    "                sum(1 for idx in (logits[\"logits\"] > pred.best_db).nonzero(as_tuple=True)[0].tolist() if token2index[idx] in [\n",
    "                    token2index[idx] for idx in (logits[\"targets\"] == 1).nonzero(as_tuple=True)[0].tolist()\n",
    "                ]) / len((logits[\"logits\"] > pred.best_db).nonzero(as_tuple=True)[0].tolist()) * 100, 2),\n",
    "\n",
    "            }\n",
    "            for ids, logits in predict.items()\n",
    "        }\n",
    "    \n",
    "    del predict,pred,metric_collections,lr_scheduler,num_training_steps,accumulate_grad_batches,optimizer,dataloaders,datasets,callbacks,data,text_encoder,label_transform,text_transform,lookups,model\n",
    "    gc.collect()\n",
    "    return {\"status\": \"Predict successfully\", \"data\": result}\n",
    "\n",
    "# Run the app\n",
    "config = Config(app=app, host=\"0.0.0.0\", port=8081)\n",
    "server = Server(config)\n",
    "server.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
